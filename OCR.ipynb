{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import re\n",
    "from imutils import rotate_bound\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model with Preprocessing, PDF-to-Image Conversion, and Training Loop\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete pipeline for Optical Character Recognition (OCR) using a combination of EasyOCR and PyTorch. It includes:\n",
    "1. **PDF-to-Image Conversion** - Converts scanned PDFs into high-resolution images for processing.  \n",
    "2. **Image Preprocessing** - Enhances image quality by applying grayscale conversion, sharpening, thresholding, and de-skewing techniques.  \n",
    "3. **Custom Dataset and DataLoader** - Defines a dataset class for loading and augmenting image data.  \n",
    "4. **Training Loop** - Implements a training loop for fine-tuning a CNN model to improve OCR accuracy.  \n",
    "5. **Text Extraction** - Uses EasyOCR to extract text from preprocessed images and saves the output in JSON format.  \n",
    "\n",
    "## Dependencies\n",
    "Make sure the following libraries are installed:\n",
    "- OpenCV\n",
    "- PyTorch\n",
    "- EasyOCR\n",
    "- NumPy\n",
    "- TQDM\n",
    "- pdf2image\n",
    "- imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=(device == 'cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration and EasyOCR Initialization\n",
    "\n",
    "### Device Setup\n",
    "The script dynamically detects whether a **CUDA-enabled GPU** is available. If a GPU is found, computations will leverage it for faster processing. Otherwise, it defaults to the **CPU**.\n",
    "\n",
    "```python\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'aadhar.v1i.yolov5pytorch/train/images'\n",
    "output_file = 'aadhar_extracted_texts.json'\n",
    "MAX_IMAGES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "        return img, img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Dataset Class Definition\n",
    "\n",
    "This section defines the `OCRDataset` class, a custom dataset class for Optical Character Recognition (OCR) tasks. The class inherits from `torch.utils.data.Dataset` and provides methods for loading and processing images.\n",
    "\n",
    "### Class: `OCRDataset`\n",
    "\n",
    "The class is designed to handle image paths and apply necessary transformations, such as grayscale conversion, resizing, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "    image_paths = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_dir, f'page_{i+1}.jpg')\n",
    "        image.save(image_path, 'JPEG')\n",
    "        image_paths.append(image_path)\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Image Conversion\n",
    "\n",
    "This section defines the `pdf_to_images` function, which converts each page of a PDF into an image and saves them in a specified output directory.\n",
    "\n",
    "### Function: `pdf_to_images(pdf_path, output_dir)`\n",
    "\n",
    "The function converts the pages of a PDF file into images, saves them as JPEG files, and returns the paths of the saved images.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Input:**\n",
    "  - `pdf_path`: The path to the PDF file that needs to be converted.\n",
    "  - `output_dir`: The directory where the converted images will be saved.\n",
    "\n",
    "- **Process:**\n",
    "  - The `convert_from_path` function from the `pdf2image` library is used to convert the PDF into a list of images, one for each page, at a resolution of 300 DPI.\n",
    "  - Each image is saved as a JPEG file in the specified output directory.\n",
    "  - The function constructs the file path for each image using the format `page_{i+1}.jpg`, where `i` is the page index.\n",
    "  - The image paths are appended to a list, which is returned as the output.\n",
    "\n",
    "- **Output:**\n",
    "  - A list of file paths for the saved images.\n",
    "\n",
    "This function can be used to preprocess PDF files by converting them into images for further processing, such as Optical Character Recognition (OCR) or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 21, 10)\n",
    "    coords = np.column_stack(np.where(thresh > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = thresh.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    resized = cv2.resize(rotated, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "This section defines the `preprocess_image` function, which applies several image processing techniques to enhance and prepare the image for tasks like Optical Character Recognition (OCR).\n",
    "\n",
    "### Function: `preprocess_image(img_path)`\n",
    "\n",
    "The function processes an image by converting it to grayscale, sharpening it, applying thresholding, correcting rotation, and resizing the image.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Input:**\n",
    "  - `img_path`: The file path of the image to be preprocessed.\n",
    "\n",
    "- **Process:**\n",
    "  - **Grayscale Conversion**: The image is converted to grayscale to simplify the processing and focus on intensity rather than color.\n",
    "  - **Sharpening**: A sharpening kernel is applied to the grayscale image using a convolution filter to enhance edges and details in the image.\n",
    "  - **Adaptive Thresholding**: Adaptive thresholding is applied to the sharpened image to produce a binary image. This step helps in separating the foreground (text) from the background.\n",
    "  - **Angle Detection**: The function calculates the rotation angle of the text in the binary image using the minimum area rectangle. This is done to detect if the image needs to be rotated for proper alignment.\n",
    "  - **Rotation Correction**: Based on the detected angle, the image is rotated so that the text is aligned correctly.\n",
    "  - **Resizing**: The image is resized by a factor of 1.5 to enhance the features and make it suitable for further analysis.\n",
    "\n",
    "- **Output:**\n",
    "  - The preprocessed image, which is rotated and resized for improved text recognition.\n",
    "\n",
    "This function is typically used to prepare images for OCR by improving the visibility of text and correcting any distortions such as skewed or rotated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,:/\\n\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "This section defines the `clean_text` function, which processes raw text by removing unwanted characters and extra spaces to improve its quality for further analysis.\n",
    "\n",
    "### Function: `clean_text(text)`\n",
    "\n",
    "The function cleans the input text by removing non-alphanumeric characters and reducing extra whitespace.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Input:**\n",
    "  - `text`: The raw text string that needs to be cleaned.\n",
    "\n",
    "- **Process:**\n",
    "  - **Removing Unwanted Characters**: The function uses regular expressions to remove characters that are not alphanumeric (letters, digits), punctuation (.,:/), newline characters (`\\n`), or spaces. This ensures that only relevant characters remain in the text.\n",
    "  - **Condensing Multiple Spaces**: It replaces multiple consecutive spaces with a single space, ensuring that the text is well-formatted and does not contain unnecessary gaps.\n",
    "  \n",
    "- **Output:**\n",
    "  - The cleaned text, with unwanted characters removed and extra spaces condensed.\n",
    "\n",
    "This function is typically used to preprocess raw text data, especially for tasks like text recognition (e.g., OCR) or text analysis, ensuring the text is in a usable and standardized format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ocr_model(dataloader, epochs=5, lr=0.001):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(64 * 160 * 160, 256),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 10)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for imgs, _ in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, torch.randint(0, 10, (imgs.size(0),), device=device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the OCR Model\n",
    "\n",
    "This section defines the `train_ocr_model` function, which sets up and trains a convolutional neural network (CNN) model for Optical Character Recognition (OCR) tasks.\n",
    "\n",
    "### Function: `train_ocr_model(dataloader, epochs=5, lr=0.001)`\n",
    "\n",
    "The function initializes and trains a simple CNN model for OCR, utilizing a given dataloader to load images and their corresponding labels.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Input:**\n",
    "  - `dataloader`: A PyTorch DataLoader that supplies batches of training images and their labels.\n",
    "  - `epochs`: The number of times the model will iterate over the entire dataset. Default is 5.\n",
    "  - `lr`: The learning rate for the optimizer. Default is 0.001.\n",
    "\n",
    "- **Process:**\n",
    "  - **Model Setup**: A CNN model is defined using two convolutional layers followed by ReLU activations and max-pooling. The model is designed to process grayscale images and output a classification of digits (from 0 to 9).\n",
    "  - **Optimizer and Loss**: The Adam optimizer is used with a learning rate of `lr`, and the Cross-Entropy loss is used for training. The optimizer is responsible for updating the model parameters during training to minimize the loss.\n",
    "  - **Training Loop**: The model undergoes training for the specified number of epochs:\n",
    "    - For each batch of images, the forward pass computes the predictions.\n",
    "    - The loss is calculated by comparing the predictions with randomly generated labels (in this case, the labels are simulated as random values between 0 and 9).\n",
    "    - The optimizer performs a backward pass to update the model’s parameters based on the loss.\n",
    "    - After each epoch, the loss for the final batch is printed to monitor the model’s progress.\n",
    "\n",
    "- **Output:**\n",
    "  - The model is trained and ready for further evaluation or inference.\n",
    "\n",
    "This function trains a basic CNN model on OCR tasks, learning to classify images of text (here, digits from 0-9). The model is trained using synthetic labels, but in a real scenario, actual labeled data would be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images():\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    image_files.sort()\n",
    "    image_files = image_files[200:400]\n",
    "    data = {\"input\": []}\n",
    "    for filename in tqdm(image_files, desc=\"Processing Images\"):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        processed_img = preprocess_image(img_path)\n",
    "        best_result = \"\"\n",
    "        for angle in [0, 90, 180, 270]:\n",
    "            rotated_img = rotate_bound(processed_img, angle)\n",
    "            results = reader.readtext(rotated_img, detail=0, paragraph=True)\n",
    "            extracted_text = ' '.join(results)\n",
    "            if len(extracted_text) > len(best_result):\n",
    "                best_result = extracted_text\n",
    "        cleaned_text = clean_text(best_result)\n",
    "        data[\"input\"].append({\n",
    "            \"image\": filename,\n",
    "            \"text_extracted\": cleaned_text\n",
    "        })\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Text extraction complete! Results saved in {output_file}\")\n",
    "image_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "image_files.sort()\n",
    "image_files = image_files[200:400]\n",
    "dataset = OCRDataset(image_files)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "train_ocr_model(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from Images\n",
    "\n",
    "This section defines the `extract_text_from_images` function, which extracts text from a batch of images in a specified directory using OCR and saves the results to a JSON file.\n",
    "\n",
    "### Function: `extract_text_from_images()`\n",
    "\n",
    "The function processes a set of images, extracts text from each using OCR, and stores the results in a structured format.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **Input:**\n",
    "  - The function loads images from the `input_folder` directory. It filters out files that are not images (`.png`, `.jpg`, `.jpeg`), sorts them, and selects a subset of images (from index 200 to 400) for processing.\n",
    "\n",
    "- **Process:**\n",
    "  - **Preprocessing**: Each image is processed using the `preprocess_image` function to enhance text visibility (e.g., by rotating, thresholding, and resizing).\n",
    "  - **Text Extraction**: For each image, the function attempts text extraction at four different rotation angles (0°, 90°, 180°, and 270°). The best result is chosen based on the longest extracted text.\n",
    "  - **Text Cleaning**: The extracted text is cleaned using the `clean_text` function to remove unwanted characters and normalize spacing.\n",
    "  - **Storing Results**: The filename and corresponding extracted text are stored in a dictionary. After processing all images, the results are saved to a JSON file (`output_file`).\n",
    "\n",
    "- **Output:**\n",
    "  - A JSON file containing the extracted text for each image.\n",
    "\n",
    "### Dataset Creation and Model Training\n",
    "\n",
    "Following the text extraction, the code creates a dataset and trains an OCR model.\n",
    "\n",
    "- **Dataset Creation**: The `OCRDataset` class is used to create a dataset from the selected image files, which are loaded into a `DataLoader` for batching and shuffling.\n",
    "  \n",
    "- **Model Training**: The `train_ocr_model` function is called to train a CNN-based model on the dataset for OCR tasks.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Load Images**: Filter and select images from the `input_folder`.\n",
    "2. **Process Each Image**: Preprocess each image, extract text at different rotations, and clean the results.\n",
    "3. **Save Results**: Store the extracted text in a JSON file.\n",
    "4. **Train Model**: Use the processed images to train an OCR model.\n",
    "\n",
    "This function combines image preprocessing, text extraction, and OCR model training, making it a comprehensive pipeline for working with OCR tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_images()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction and Model Optimization\n",
    "\n",
    "This section runs the `extract_text_from_images` function to process images, extract text using OCR, and save the results in a JSON file.\n",
    "\n",
    "### Steps:\n",
    "1. **Text Extraction**: The function processes a subset of images from the `input_folder`, extracts text using OCR, and stores the results in `output_file`.\n",
    "2. **Optimize GPU Memory**: After extraction, `torch.cuda.empty_cache()` is called to free up unused GPU memory.\n",
    "\n",
    "This process allows for efficient text extraction and optimizes memory usage during training or inference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
